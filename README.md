# NLP and Spam Detection

This project involves developing a Skip-Gram model with negative sampling for the generation of high-quality word embeddings and implementing spam detection models using Multinomial & Bernoulli Naive Bayes classifiers.

## Table of Contents
- [Introduction](#introduction)
- [Usage](#usage)
- [Results](#results)

## Introduction

The aim of this project is to enhance the detection of spam messages using Natural Language Processing (NLP) techniques. The project is divided into two main parts:
1. **Skip-Gram Embedding**: Developing a Skip-Gram model with negative sampling for generating high-quality word embeddings.
2. **Spam Detection**: Implementing and comparing Multinomial and Bernoulli Naive Bayes classifiers for spam detection.

## Usage

### Skip-Gram Embedding

- Navigate to the notebooks directory.
- Open and run the `skip_gram_embedding.ipynb` notebook to train the Skip-Gram model and generate word embeddings.

### Spam Detection

- Navigate to the notebooks directory.
- Open and run the `spam.ipynb` notebook to train and evaluate the Multinomial and Bernoulli Naive Bayes classifiers on the spam dataset.

## Results

- The Skip-Gram model produces high-quality word embeddings, which can be visualized and used for various NLP tasks.
- The spam detection models are evaluated for their performance, achieving an accuracy of 98.85%, demonstrating the effectiveness of the Naive Bayes classifiers in detecting spam messages.
